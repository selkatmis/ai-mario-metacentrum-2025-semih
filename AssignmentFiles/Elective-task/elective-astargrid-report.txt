# Astar Elective Task Short Report

#1 Experiment Overview
We evaluated dynamic heuristic weighting for the A* Grid agent. The time-to-finish weight is adjusted as progress increases across the level, allowing both increasing and decreasing strategies:
1. Increasing: startWeight < endWeight, the agent relies more on the heuristic near the goal.
2. Decreasing: startWeight > endWeight, the agent relies more on the heuristic early and relaxes near the goal.

Parameters:
- searchSteps fixed at 3
- startWeight and endWeight varied across the ranges defined in Metacentrum-Scripts/metascript-dynamic-weight.sh and Experiment-Results/MFF A-Star/dynamic-weighting/metacentrum-batches.txt (batches 1–16)
- exponent values tested: 0.5, 1.0, 1.5, 2.0, 2.5, 3.0
- dynamic weighting enabled for all runs

The experiment covered:
- 15 original levels
- first 100 krys levels
- first 100 patternCount levels
- At least 100 parameter combinations were executed by splitting the sweep into 16 batches (batch1–batch16).

#2 Data Provenance
Runs were executed on MetaCentrum using semih_dynamic_weight.sh, with batches submitted via metascript-dynamic-weight.sh and metacentrum-batches.txt (batch1–batch16). Each batch copied its outputs to results/astar-grid-dynamic/ on the cluster.

Collected results were merged into the repository under Experiment-Results/astar-grid-dynamic:
- astar-grid-dynamic-detail-full.csv
- astar-grid-dynamic-summary-full.csv

The Java benchmark used:
mff.agents.benchmark.AstarGridDynamicWeightBenchmark
^ which reads SEARCH_STEPS, START_WEIGHTS, END_WEIGHTS, EXPONENTS from environment variables to control each batch.

#3 Key Findings
Across exponents and weight schedules, dynamic weighting consistently achieved very high win rates on the tested level packs. The differences between strategies are most visible in runtime and nodes evaluated.

Main observations
- Win rate: broad plateau at 98–100% across most of the grid. The only notable dips are for low startWeight with high endWeight (start=0.6 with end=1.2–1.5 gives 93–96%). Both increasing and decreasing schedules achieve near-perfect success elsewhere.
- Runtime: the valley is on the decreasing side (startWeight > endWeight). Best region is start=1.3–1.7 with end=0.8–1.3, where average times are roughly 0.9–1.4 s. Increasing schedules with large gaps (low start, high end) run much slower (often 5–8.5 s).
- Nodes evaluated: mirrors runtime. Minimum search effort appears in the same decreasing band (about 67k–120k nodes), while low start with high endWeight exceeds 300k–450k nodes.
- Practical recommendation for this agent and level set: startWeight in the 1.4–1.6 range and endWeight around 0.8–1.0. This setting kept win rate at 99–100% and produced runtimes near 1.0–1.3 s with low node counts.

Overall, decreasing weights with startWeight in the 1.2–1.6 range and endWeight at or below 1.0 offered the best runtime without sacrificing success. Increasing weights are safe and strong but not consistently faster.

#4 Visualizations
One heatmap per metric in AssignmentFiles/Elective-task/dynamic-weighting-plots:
- dynamic-weight-winRate.png
- dynamic-weight-avgRunTime.png
- dynamic-weight-avgNodesEvaluated.png
These plots summarize performance over startWeight and endWeight and reflect the aggregate trends described above.

#5 Follow-up Actions
Evaluate a small control set with dynamic weighting disabled to quantify the net benefit relative to a static time-to-finish weight at the best-performing value.
We could also sweep through searchSteps beyond 3 to see if the preferred dynamic schedule shifts with deeper lookahead.
Also testing out much more weights could give us more data on the heat map.
Consider summarizing per-pack performance (original vs. krys vs. patternCount) to highlight any pack-specific differences in preferred schedules.
